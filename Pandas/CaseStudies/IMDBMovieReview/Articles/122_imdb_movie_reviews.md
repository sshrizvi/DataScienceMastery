# <picture><source srcset="https://pandas.pydata.org/static/img/pandas_mark_white.svg" type="image/webp"><img src="https://pandas.pydata.org/static/img/pandas_mark_white.svg" width="32" height="32"></picture> Pandas for Data Science

> [!TIP]  
> Link to Previous Article  
> ðŸ¡¸ [Time Series Analysis on Google Stock Data](../../TimeSeriesAnalysis/Articles/121_casestudy02_time_series_analysis.md)

## ðŸŽ¬ IMDB Movie Reviews Analysis

> [!IMPORTANT]  
> Link to Related Notebooks of this article for practical implementation.
> [Notebooks](../Notebooks/)  
> Link to Related Data of this article for practical implementation.
> [Data](../Data/)  

This repository contains the full workflow and assets for my **NLPâ€¯caseâ€‘study on 50â€¯000 IMDB movie reviews**.  
The goal is to walk from raw HTMLâ€‘laden text to clean features that let us explore sentiment patterns visually
and feed future ML models.

### Environment

| Library | Purpose |
|---------|---------|
| `pandas`, `numpy` | Data Wrangling |
| `nltk`, `textblob` | Tokenisation & Linguistic Helpers |
| `wordcloud` | Text Visualisation |
| `matplotlib`, `seaborn` | Plotting |
| `scikitâ€‘learn` | Vectorisation (`CountVectorizer`) & Dimensionality Reduction (`PCA`) |

Create the environment with:

```bash
pip install -r requirements.txt
```

*(See the `environment.yml` or `requirements.txt` for exact versions.)*

### Quick start

1. **Clone** the repo and `cd` into it.  
2. Place *IMDBMovieReviewsDataset.csv* inside `data/` (or let the notebook download from Kaggle).  
3. Launch the notebook:  

   ```bash
   jupyter notebook notebook/IMDB_Movie_Reviews_Analysis.ipynb
   ```

### Methodology

| Phase | Key steps |
|-------|-----------|
| **1. Loading** | Read the Kaggle CSV (`50â€¯000` labelled reviews). |
| **2. Cleaning** | Lowerâ€‘casing, trimming, stripping HTML/URLs, expanding contractions, removing punctuation & special chars. |
| **3. Preâ€‘processing** | Tokenise with NLTK, drop English stopâ€‘words, rejoin into a *processed_review* column. |
| **4. Feature engineering** | *Text length* (chars), *word count*, **uni/bi/triâ€‘grams** frequency tables. |
| **5. EDA** | Wordâ€‘clouds for positive vs. negative subsets to surface dominant terms. |
| **6. Vectorisation** | Bagâ€‘ofâ€‘Words (topâ€¯5â€¯000 tokens) via `CountVectorizer`. |
| **7. Projection** | 2â€‘D **PCA** scatterâ€‘plot coloured by sentiment â€” quick sanityâ€‘check of separability. |

### Results & insights

* Wordâ€‘clouds show positive reviews emphasise *story*, *character*, *time* whereas negatives highlight *movie*, *even*, *one* â€“ indicating differing topical focuses.  
* PCA projection reveals partial clustering: positive and negative points form mixed but discernible regions, hinting that linear models plus richer features may achieve good separation.
* Feature tables (uni/bi/triâ€‘grams) ready the dataset for downstream classifiers (LogReg, SVM, BERTÂ â€¦), which are the logical next step.

> [!TIP]  
> Link to Next Article  
> ðŸ¡º [Data Visualization - Matplotlib](../../../../DataVisualization/Articles/123_matplotlib.md)