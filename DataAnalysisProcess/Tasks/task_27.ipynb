{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Task 27 - (Article 129)** [![Static Badge](https://img.shields.io/badge/Open%20in%20Colab%20-%20orange?style=plastic&logo=googlecolab&labelColor=grey)](https://colab.research.google.com/github/sshrizvi/DataScienceMastery/blob/main/DataAnalysisProcess/Tasks/task_27.ipynb)\n",
        "\n",
        "|üî¥ **WARNING** üî¥|\n",
        "|:-----------:|\n",
        "| If you have not studied article 129. Do checkout the articles before attempting the task. |\n",
        "| Here is [Data Gathering](../Articles/129_data_gathering.md) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ **Q01 : Export SQL Tables to Excel with Multiple Sheets**\n",
        "\n",
        "1. **Dataset:** SQL Database File : [File](../Resources/Data/sales.sql)\n",
        "\n",
        "2. **Task Description:**\n",
        "\n",
        "   * Read the data from the given SQL file.\n",
        "   * Identify the three tables present in the data:\n",
        "\n",
        "     * `invoices`\n",
        "     * `order_leads`\n",
        "     * `sales`\n",
        "   * Create an Excel file to store the extracted data.\n",
        "   * Create **three separate sheets** in the Excel file.\n",
        "   * Store each table‚Äôs data in its corresponding Excel sheet.\n",
        "\n",
        "3. **Expected Outcome:**\n",
        "   An Excel file containing three sheets ‚Äî `invoices`, `order_leads`, and `sales` ‚Äî each populated with data from the respective SQL tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# Create SQLALchemy Engine\n",
        "engine = create_engine('mysql+mysqlconnector://root:62292003@localhost:3306/sales_db')\n",
        "\n",
        "# Read Tables from MySQL\n",
        "invoices_df = pd.read_sql('invoices', con=engine)\n",
        "order_leads_df = pd.read_sql('order_leads', con=engine)\n",
        "sales_df = pd.read_sql('sales', con=engine)\n",
        "\n",
        "# Ensure Directory\n",
        "output_path = '../Resources/Exports/sales.xlsx'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "# Export Data Into Excel Sheets\n",
        "with pd.ExcelWriter(output_path) as writer:\n",
        "    invoices_df.to_excel(writer, sheet_name='invoices', index=False)\n",
        "    order_leads_df.to_excel(writer, sheet_name='order_leads', index=False)\n",
        "    sales_df.to_excel(writer, sheet_name='sales', index=False)\n",
        "    \n",
        "# Verify Export\n",
        "assert invoices_df.shape == pd.read_excel(output_path, sheet_name='invoices').shape\n",
        "assert order_leads_df.shape == pd.read_excel(output_path, sheet_name='order_leads').shape\n",
        "assert sales_df.shape == pd.read_excel(output_path, sheet_name='sales').shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ **Q02 : Collect City Data via GeoDB Cities API and Store in SQL**\n",
        "\n",
        "1. **Dataset:** GeoDB Cities API [*(RapidAPI ‚Äì GeoDB Cities)*](https://rapidapi.com/wirefreethought/api/geodb-cities)\n",
        "\n",
        "2. **Task Description:**\n",
        "\n",
        "   * Access the GeoDB Cities API from the provided RapidAPI platform.\n",
        "   * Select appropriate API routes to retrieve city data for different countries.\n",
        "   * Fetch city data for all available countries using the API.\n",
        "   * Consolidate the retrieved data into a single structured dataframe.\n",
        "   * Store the complete dataframe into a CSV file.\n",
        "   * Use the available free subscription plan and adjust data collection to comply with its limitations.\n",
        "\n",
        "3. **Expected Outcome:**\n",
        "   A CSV file containing a table populated with city data collected from multiple countries using the GeoDB Cities API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_cities_data(url: str,\n",
        "                      params: dict,\n",
        "                      headers: dict,\n",
        "                      calls: int):\n",
        "    '''Fetch Cities Data from GeoDB Rapid API.'''\n",
        "    \n",
        "    # Ensure Directory\n",
        "    output_path = '../Resources/Exports/geodb_cities.csv'\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    # Removing File for Fresh Start\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "    \n",
        "    # Make Calls\n",
        "    with tqdm.tqdm(total=100) as pbar:\n",
        "        for call in range(calls):\n",
        "            \n",
        "            # Hit API\n",
        "            response = requests.get(url=url,\n",
        "                                    params=params,\n",
        "                                    headers=headers)\n",
        "            \n",
        "            data = response.json()['data']\n",
        "            geodb_cities = pd.DataFrame(data)\n",
        "            \n",
        "            # Write to CSV Incrementally\n",
        "            geodb_cities.to_csv(\n",
        "                output_path,\n",
        "\t\t\t\tmode='a',\n",
        "\t\t\t\tindex=False,\n",
        "\t\t\t\theader=(call == 0)\n",
        "\t\t\t)\n",
        "            \n",
        "            # Update Offset\n",
        "            params['offset'] += params['limit']\n",
        "            \n",
        "            # Delay\n",
        "            time.sleep(1)\n",
        "            \n",
        "            # Update Progress\n",
        "            pbar.update(100 / calls)\n",
        "        \n",
        "    geodb_cities_df = pd.read_csv(output_path)\n",
        "    print(f'Fetched {geodb_cities_df.shape[0]} cities data.')\n",
        "    return geodb_cities_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets test our `fetch_cities_data` method for `991` API Calls (*Left in my FREE Plan*).  \n",
        "If worked fine, it will give us data of `9910` cities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities\"\n",
        "params = {\n",
        "\t\"offset\": 0,\n",
        "\t\"limit\": 10\n",
        "}\n",
        "headers = {\n",
        "\t\"x-rapidapi-key\": \"479156b394mshd7c5f0abb628d8fp1fd467jsn25972805f0ef\",\n",
        "\t\"x-rapidapi-host\": \"wft-geo-db.p.rapidapi.com\"\n",
        "}\n",
        "geodb_cities_df = fetch_cities_data(url=URL,\n",
        "                                    params=params,\n",
        "                                    headers=headers,\n",
        "                                    calls=991)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>wikiDataId</th>\n",
              "      <th>type</th>\n",
              "      <th>city</th>\n",
              "      <th>name</th>\n",
              "      <th>country</th>\n",
              "      <th>countryCode</th>\n",
              "      <th>region</th>\n",
              "      <th>regionCode</th>\n",
              "      <th>regionWdId</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3350606</td>\n",
              "      <td>Q24668</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Aixirivall</td>\n",
              "      <td>Aixirivall</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Sant Juli√† de L√≤ria</td>\n",
              "      <td>06</td>\n",
              "      <td>Q24282</td>\n",
              "      <td>42.462450</td>\n",
              "      <td>1.502090</td>\n",
              "      <td>1025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3216144</td>\n",
              "      <td>Q24656</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Aixovall</td>\n",
              "      <td>Aixovall</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Sant Juli√† de L√≤ria</td>\n",
              "      <td>06</td>\n",
              "      <td>Q24282</td>\n",
              "      <td>42.476358</td>\n",
              "      <td>1.489492</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3406038</td>\n",
              "      <td>Q4699394</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Aix√†s</td>\n",
              "      <td>Aix√†s</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Sant Juli√† de L√≤ria</td>\n",
              "      <td>06</td>\n",
              "      <td>Q24282</td>\n",
              "      <td>42.486389</td>\n",
              "      <td>1.467222</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>397</td>\n",
              "      <td>Q1863</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Andorra la Vella</td>\n",
              "      <td>Andorra la Vella</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Andorra la Vella</td>\n",
              "      <td>07</td>\n",
              "      <td>Q2522163</td>\n",
              "      <td>42.507222</td>\n",
              "      <td>1.522222</td>\n",
              "      <td>24042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3360277</td>\n",
              "      <td>Q24475</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Ansalonga</td>\n",
              "      <td>Ansalonga</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Ordino</td>\n",
              "      <td>05</td>\n",
              "      <td>Q24272</td>\n",
              "      <td>42.568443</td>\n",
              "      <td>1.521571</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3341362</td>\n",
              "      <td>Q24551</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Any√≥s</td>\n",
              "      <td>Any√≥s</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>La Massana</td>\n",
              "      <td>04</td>\n",
              "      <td>Q24276</td>\n",
              "      <td>42.534592</td>\n",
              "      <td>1.541650</td>\n",
              "      <td>1006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3361293</td>\n",
              "      <td>Q24478</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Arans</td>\n",
              "      <td>Arans</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Ordino</td>\n",
              "      <td>05</td>\n",
              "      <td>Q24272</td>\n",
              "      <td>42.583333</td>\n",
              "      <td>1.516667</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>866</td>\n",
              "      <td>Q24554</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Arinsal</td>\n",
              "      <td>Arinsal</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>La Massana</td>\n",
              "      <td>04</td>\n",
              "      <td>Q24276</td>\n",
              "      <td>42.571980</td>\n",
              "      <td>1.484700</td>\n",
              "      <td>1419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3394034</td>\n",
              "      <td>Q24650</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Aubiny√†</td>\n",
              "      <td>Aubiny√†</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Sant Juli√† de L√≤ria</td>\n",
              "      <td>06</td>\n",
              "      <td>Q24282</td>\n",
              "      <td>42.452800</td>\n",
              "      <td>1.493000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3292906</td>\n",
              "      <td>Q24641</td>\n",
              "      <td>CITY</td>\n",
              "      <td>Bixessarri</td>\n",
              "      <td>Bixessarri</td>\n",
              "      <td>Andorra</td>\n",
              "      <td>AD</td>\n",
              "      <td>Sant Juli√† de L√≤ria</td>\n",
              "      <td>06</td>\n",
              "      <td>Q24282</td>\n",
              "      <td>42.482511</td>\n",
              "      <td>1.458608</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id wikiDataId  type              city              name  country  \\\n",
              "0  3350606     Q24668  CITY        Aixirivall        Aixirivall  Andorra   \n",
              "1  3216144     Q24656  CITY          Aixovall          Aixovall  Andorra   \n",
              "2  3406038   Q4699394  CITY             Aix√†s             Aix√†s  Andorra   \n",
              "3      397      Q1863  CITY  Andorra la Vella  Andorra la Vella  Andorra   \n",
              "4  3360277     Q24475  CITY         Ansalonga         Ansalonga  Andorra   \n",
              "5  3341362     Q24551  CITY             Any√≥s             Any√≥s  Andorra   \n",
              "6  3361293     Q24478  CITY             Arans             Arans  Andorra   \n",
              "7      866     Q24554  CITY           Arinsal           Arinsal  Andorra   \n",
              "8  3394034     Q24650  CITY           Aubiny√†           Aubiny√†  Andorra   \n",
              "9  3292906     Q24641  CITY        Bixessarri        Bixessarri  Andorra   \n",
              "\n",
              "  countryCode               region regionCode regionWdId   latitude  \\\n",
              "0          AD  Sant Juli√† de L√≤ria         06     Q24282  42.462450   \n",
              "1          AD  Sant Juli√† de L√≤ria         06     Q24282  42.476358   \n",
              "2          AD  Sant Juli√† de L√≤ria         06     Q24282  42.486389   \n",
              "3          AD     Andorra la Vella         07   Q2522163  42.507222   \n",
              "4          AD               Ordino         05     Q24272  42.568443   \n",
              "5          AD           La Massana         04     Q24276  42.534592   \n",
              "6          AD               Ordino         05     Q24272  42.583333   \n",
              "7          AD           La Massana         04     Q24276  42.571980   \n",
              "8          AD  Sant Juli√† de L√≤ria         06     Q24282  42.452800   \n",
              "9          AD  Sant Juli√† de L√≤ria         06     Q24282  42.482511   \n",
              "\n",
              "   longitude  population  \n",
              "0   1.502090        1025  \n",
              "1   1.489492          69  \n",
              "2   1.467222           0  \n",
              "3   1.522222       24042  \n",
              "4   1.521571           0  \n",
              "5   1.541650        1006  \n",
              "6   1.516667           0  \n",
              "7   1.484700        1419  \n",
              "8   1.493000           0  \n",
              "9   1.458608           0  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "geodb_cities_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéØ **Q03 : Web Scraping Smartphone Data from Flipkart and Export to JSON**\n",
        "\n",
        "1. **Dataset:** Flipkart Smartphones Listing [Link](https://www.flipkart.com/search?q=smartphones)\n",
        "\n",
        "2. **Task Description:**\n",
        "\n",
        "   * Access the Flipkart smartphones search page using the given URL.\n",
        "   * Extract the following details for each listed smartphone:\n",
        "\n",
        "     * Image URL\n",
        "     * Phone name\n",
        "     * Average rating\n",
        "     * Total number of ratings\n",
        "     * Total number of reviews\n",
        "     * Discounted price\n",
        "     * Actual price\n",
        "   * Navigate through all available pages using pagination to collect data for every listed smartphone.\n",
        "   * Introduce a delay of **2‚Äì3 seconds** after each page request to avoid access restrictions.\n",
        "   * Compile all extracted information into a structured format.\n",
        "   * Save the complete collected data into a **CSV file**.\n",
        "\n",
        "3. **Expected Outcome:**\n",
        "   A CSV file containing structured data for all smartphones listed on Flipkart, including pricing, ratings, reviews, and image details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def scrape_smartphone_details(url: str,\n",
        "                              params: dict,\n",
        "                              headers: dict,\n",
        "                              pages: int\n",
        "                              ):\n",
        "    '''Scrape Smartphone Details from Flipkart.'''\n",
        "\n",
        "    # Ensure Directory\n",
        "    output_path = '../Resources/Exports/smartphones.csv'\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    \n",
        "    # Remove File for Fresh Start\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "\n",
        "    # Scrape Data\n",
        "    with tqdm.tqdm(total=pages, desc='Fetching Smartphones Data') as pbar:\n",
        "        for page in range(1, pages + 1):\n",
        "\n",
        "            # Set Page\n",
        "            params['page'] = page\n",
        "\n",
        "            # Hit URL\n",
        "            try:\n",
        "                webpage = requests.get(url=url,\n",
        "                                    params=params,\n",
        "                                    headers=headers)\n",
        "                webpage.raise_for_status()\n",
        "                \n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f'Timeout : Skipping Page {page}')\n",
        "                continue\n",
        "\n",
        "            # Make Soup\n",
        "            soup = BeautifulSoup(webpage.text, features='html.parser')\n",
        "\n",
        "            # Find Smartphones\n",
        "            smartphones = soup.find_all(name='div', class_='jIjQ8S')\n",
        "\n",
        "            # Declare Temporary Dictionary\n",
        "            smartphones_details = dict(\n",
        "                image_url=[],\n",
        "                phone_name=[],\n",
        "                average_rating=[],\n",
        "                no_of_ratings=[],\n",
        "                no_of_reviews=[],\n",
        "                discounted_price=[],\n",
        "                actual_price=[]\n",
        "            )\n",
        "\n",
        "            # Extract Details\n",
        "            for smartphone in smartphones:\n",
        "                try:\n",
        "                    smartphones_details['image_url'].append(\n",
        "                        smartphone.find(name='img', class_='UCc1lI')['src']\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['image_url'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['phone_name'].append(\n",
        "                        smartphone.find(name='div', class_='RG5Slk').text\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['phone_name'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['average_rating'].append(\n",
        "                        smartphone.find(name='div', class_='MKiFS6').text\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['average_rating'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['no_of_ratings'].append(\n",
        "                        smartphone.find(name='span', class_='PvbNMB').text.split('&')[0].strip()\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['no_of_ratings'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['no_of_reviews'].append(\n",
        "                        smartphone.find(name='span', class_='PvbNMB').text.split('&')[1].strip()\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['no_of_reviews'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['discounted_price'].append(\n",
        "                        smartphone.find(name='div', class_='hZ3P6w DeU9vF').text\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['discounted_price'].append(np.nan)\n",
        "                try:\n",
        "                    smartphones_details['actual_price'].append(\n",
        "                        smartphone.find(name='div', class_='kRYCnD gxR4EY').text\n",
        "                    )\n",
        "                except:\n",
        "                    smartphones_details['actual_price'].append(np.nan)\n",
        "            \n",
        "            # Write to CSV Incrementally\n",
        "            smartphone_df = pd.DataFrame(smartphones_details)\n",
        "            smartphone_df.to_csv(\n",
        "                output_path,\n",
        "                mode='a',\n",
        "                index=False,\n",
        "                header=(page == 1)\n",
        "            )\n",
        "            \n",
        "            # Delay\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Update Progress\n",
        "            pbar.update(1)\n",
        "            \n",
        "    smartphones_df = pd.read_csv(output_path)\n",
        "    print(f'Scraped {smartphones_df.shape[0]} Smartphones Details.')\n",
        "    return smartphones_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets run the `scrape_smartphone_details` method for `50` pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "URL = 'https://www.flipkart.com/search'\n",
        "params = {'q': 'smartphones', 'page': 1}\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-IN,en;q=0.9\"\n",
        "}\n",
        "\n",
        "smartphones_df = scrape_smartphone_details(url=URL,\n",
        "                                           params=params,\n",
        "                                           headers=HEADERS,\n",
        "                                           pages=50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "learningenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
